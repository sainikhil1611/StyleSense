{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7ozPPfQlwHFVDGwY5qUhL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Torchvision: Base library for Computer Vision\n",
        "\n",
        "* `torchvision.datasets` - get datasets and data loading functions for computer vision.\n",
        "* `torchvision.models` - get pretrained computer vision models that you can leverage for your own problems.\n",
        "* `torchvision.transforms` - functions for manipulating your vision data to be suitable for use with an ML model.\n",
        "* `torch.utils.data.Dataset` - Base dataset class for PyTorch.\n",
        "* `torch.utils.data.Dataloader` - Creates a Python iterable over a dataset."
      ],
      "metadata": {
        "id": "a-vUETc_FXBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Computer Vision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Torchvision version:\", torchvision.__version__)"
      ],
      "metadata": {
        "id": "tE6Xm5osGjmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting a Dataset\n",
        "The dataset we will be using is **FashionMNIST** from torchvision.datasets."
      ],
      "metadata": {
        "id": "ogoO7AzsHB35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root = \"data\", # where to download data to?\n",
        "    train = True, # do we want the training dataset. If false, testing dataset\n",
        "    download = True, # do we want to download\n",
        "    transform = ToTensor(), # how to transform the data\n",
        "    target_transform = None # how to transform the labels\n",
        ")\n",
        "\n",
        "# Testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root = \"data\", # where to download data to?\n",
        "    train = False, # do we want the training dataset. If false, testing dataset\n",
        "    download = True, # do we want to download\n",
        "    transform = ToTensor(), # how to transform the data\n",
        "    target_transform = None # how to transform the labels\n",
        ")"
      ],
      "metadata": {
        "id": "duNHqslFHVNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "id": "Tzf6OpZPJIdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First training example\n",
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "id": "ARtre9oLJcgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "id": "rA6lLqllJnAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_to_idx = train_data.class_to_idx\n",
        "class_to_idx"
      ],
      "metadata": {
        "id": "YzKTMHusJw2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.targets # The labels"
      ],
      "metadata": {
        "id": "GqAq3H-AJzcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of our image\n",
        "print(f\"Image shape: {image.shape}\")\n",
        "print(f\"Image label: {class_names[label]}\")"
      ],
      "metadata": {
        "id": "PUDbCNVVKAdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our data\n",
        "image, label = train_data[0]\n",
        "print(f\"Image shape: {image.shape}\") # Color channels may be first or last based on the library\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "pq3EwCZIKKAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting more images\n",
        "torch.manual_seed(42)\n",
        "fig = plt.figure(figsize=(9, 9))\n",
        "rows, cols = 4, 4\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    image, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "semkMagxLZtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare DataLoader\n",
        "\n",
        "DataLoader turns our dataset into a Python iterable. We want to turn our data into mini-batches.\n",
        "\n",
        "We do this because:\n",
        "1. It is more computationally efficient as computer hardware may not be able to store 60000 images in one hit. So, we break it down into 32 images at a time.\n",
        "2. It gives our neural network more chances to update its gradients per epoch."
      ],
      "metadata": {
        "id": "5aBhOPuTMCq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Setting up the batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Turn dataset into iterables (batches)\n",
        "train_dataloader = DataLoader(\n",
        "    dataset = train_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = True, # Shuffles the data in batches\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset = test_data,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    shuffle = False, # Don't want to shuffle the testing data\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "id": "1HoIzPJ2dVMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"DataLoaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
      ],
      "metadata": {
        "id": "tygS1zfXfWKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
        "train_features_batch.shape, train_labels_batch.shape\n",
        "# batch size, color channels, height, width"
      ],
      "metadata": {
        "id": "YxY9w2M2f4Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample\n",
        "torch.manual_seed(42)\n",
        "random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\n",
        "image, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
        "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "plt.title(class_names[label])\n",
        "plt.axis(\"off\");\n",
        "print(f\"Image size: {image.shape}\")\n",
        "print(f\"Label: {label}, label size: {label.shape}\")"
      ],
      "metadata": {
        "id": "N7w94uk0fjHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model 0: Build a baseline model\n",
        "\n",
        "Simple model you will try and improve upon with subsequent models and experiments."
      ],
      "metadata": {
        "id": "DUJaTH7BjmuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # Combines into a single vector\n",
        "\n",
        "# Get a single sample\n",
        "x = train_features_batch[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x)\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape}\") # [color_channels, height, width]\n",
        "print(f\"Shape after flattening: {output.shape}\") # [color_channels, height*width]"
      ],
      "metadata": {
        "id": "LQ5mWozCggFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class FashionMNISTModelV0(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_shape: int,\n",
        "                 hidden_units: int,\n",
        "                 output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "BI0pTvKVkHO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create an instance of FashionMNISTModelV0\n",
        "model_0 = FashionMNISTModelV0(\n",
        "    input_shape = 28*28,\n",
        "    hidden_units = 10,\n",
        "    output_shape = len(class_names)\n",
        ").to(\"cpu\")\n",
        "\n",
        "model_0"
      ],
      "metadata": {
        "id": "NNJPKKZwlb2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_x = torch.rand([1, 1, 28, 28])\n",
        "model_0(dummy_x)"
      ],
      "metadata": {
        "id": "DIa4j2VFl9LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "98uAL1oemGhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Download helper functions from Learn PyTorch repo\n",
        "if Path(\"helper_functions.py\").is_file():\n",
        "  print(\"helper_functions.py already exists, skipping download\")\n",
        "else:\n",
        "  print(\"Downloading helper_functions.py\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/refs/heads/main/helper_functions.py\") # Always get the raw file\n",
        "  with open(\"helper_functions.py\", \"wb\") as f: # write binary\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "id": "FFyV-n_bmtxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import accuracy metric from helper_functions.py\n",
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Create a loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "NEfsorcEn7dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def print_train_time(start: float, # Syntax to indicate datatype\n",
        "                     end: float,\n",
        "                     device: torch.device = None):\n",
        "    \"\"\"Prints difference between start and end time.\"\"\"\n",
        "    total_time = end - start\n",
        "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
        "    return total_time"
      ],
      "metadata": {
        "id": "YrHgA1i6oZuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer()\n",
        "end_time = timer()\n",
        "print_train_time(start=start_time, end=end_time, device=\"cpu\")"
      ],
      "metadata": {
        "id": "oX6BEQZ-pI6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop\n",
        "1. Loop through epochs\n",
        "2. Loop through training batches, perform training steps, calculate the train loss per batch.\n",
        "3. Loop through testing batches, perform testing steps, calculate the test loss per batch.\n",
        "4. Print out what's happening.\n",
        "5. Time it all."
      ],
      "metadata": {
        "id": "nyD-cffkpSYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataloader.dataset), len(train_dataloader)"
      ],
      "metadata": {
        "id": "LWvaVnLQtCWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Set the seed and start the timer\n",
        "torch.manual_seed(42)\n",
        "train_time_start_on_cpu = timer()\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 3 # We keep it small for faster training time for experiments\n",
        "\n",
        "# Create training and test loop\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "\n",
        "    # Add a loop to loop through the training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader): # enumerate is used to keep track of value and index in an iterable\n",
        "      model_0.train()\n",
        "\n",
        "      # Forward Pass\n",
        "      y_pred = model_0(X)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      # Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      # Optimizer Step\n",
        "      optimizer.step() # Our model updates the model for every batch.\n",
        "\n",
        "      # Print out what's happening\n",
        "      if batch % 400 == 0:\n",
        "        print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    ### Testing\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model_0.eval()\n",
        "    with torch.inference_mode():\n",
        "      for X_test, y_test in test_dataloader:\n",
        "        # Forward Pass\n",
        "        test_pred = model_0(X_test)\n",
        "\n",
        "        # Calculate Loss\n",
        "        test_loss += loss_fn(test_pred, y_test)\n",
        "\n",
        "        # Calculate Accuracy\n",
        "        test_acc += accuracy_fn(y_true=y_test, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "      # Calculate the test loss average per batch\n",
        "      test_loss /= len(test_dataloader)\n",
        "      test_acc /= len(test_dataloader)\n",
        "      print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}\")\n",
        "\n",
        "    # Calculate training time\n",
        "    train_time_end_on_cpu = timer()\n",
        "    total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
        "                                                end=train_time_end_on_cpu,\n",
        "                                                device=str(next(model_0.parameters()).device))"
      ],
      "metadata": {
        "id": "HfqD6Q4_pqMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(next(model_0.parameters()).device)"
      ],
      "metadata": {
        "id": "CCM_j4fQt-EH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Make predictions and get Model 0 results"
      ],
      "metadata": {
        "id": "Yoo7Hzl9u0PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "  \"\"\"Return a dictionary containing results of model predictions on data_loader.\"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(data_loader):\n",
        "      # Make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "          \"model_loss\": loss.item(),\n",
        "          \"model_acc\": acc}\n",
        "\n",
        "# Calculate model 0 results on test dataset\n",
        "model_0_results = eval_model(model=model_0,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "A-iStCYKvVL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Setup device agnostic code"
      ],
      "metadata": {
        "id": "00b8t4nrwtr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "x7oj1OpSxpPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "Nsxuj5seyEpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Model 1: Building a model with non-linearity"
      ],
      "metadata": {
        "id": "Q5-mLtQNyIxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNISTModelV1(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_shape: int,\n",
        "                 hidden_units: int,\n",
        "                 output_shape: int):\n",
        "        super().__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "            nn.Flatten(), # flatten inputs into a single vector\n",
        "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.layer_stack(x)"
      ],
      "metadata": {
        "id": "uXu_eY0zyWKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of model_1\n",
        "torch.manual_seed(42)\n",
        "model_1 = FashionMNISTModelV1(input_shape=28*28, # output of flatten layer after our 28*28 image goes in\n",
        "                            hidden_units=10,\n",
        "                            output_shape=len(class_names)).to(device)\n",
        "next(model_1.parameters()).device"
      ],
      "metadata": {
        "id": "L7KLDZRAzBXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Create a loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer\n",
        "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "4fzR_jKZzXPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functionizing training and testing loops\n",
        "* Training loop - `train_step()`\n",
        "* Testing loop - `test_step()`"
      ],
      "metadata": {
        "id": "SfII3O3Vzp2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    ### Training mode\n",
        "    model.train()\n",
        "\n",
        "    # Add a loop to loop through the training batches\n",
        "    for batch, (X, y) in enumerate(data_loader): # enumerate is used to keep track of value and index in an iterable\n",
        "\n",
        "      # Put data on target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Forward Pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss\n",
        "      train_acc = accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "      # Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "\n",
        "      # Optimizer Step\n",
        "      optimizer.step() # Our model updates the model for every batch.\n",
        "\n",
        "    # Divide total train loss and acc by length of train dataloader (average loss per batch)\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train acc: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "NbLuWpK-zw-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model: torch.nn.Module,\n",
        "              data_loader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    ### Testing mode\n",
        "    model.eval()\n",
        "\n",
        "    # Turn on inference mode\n",
        "    with torch.inference_mode():\n",
        "\n",
        "      for X, y in data_loader:\n",
        "\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        test_pred = model(X)\n",
        "\n",
        "        # Calculate loss\n",
        "        test_loss += loss_fn(test_pred, y)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "      # Scale loss and acc to find the average loss/acc per batch\n",
        "      test_loss /= len(data_loader)\n",
        "      test_acc /= len(data_loader)\n",
        "      print(f\"Test loss: {test_loss:.5f} | Test acc: {test_acc:.2f}%\\n\")\n"
      ],
      "metadata": {
        "id": "pqjfMuq21kaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Measure Time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_on_gpu = timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs = 3\n",
        "\n",
        "# Create a optimization and evaluation loop using train_step() and test_step()\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    train_step(model = model_1,\n",
        "               data_loader = train_dataloader,\n",
        "               loss_fn = loss_fn,\n",
        "               optimizer = optimizer,\n",
        "               accuracy_fn = accuracy_fn,\n",
        "               device = device)\n",
        "\n",
        "    test_step(model = model_1,\n",
        "              data_loader = test_dataloader,\n",
        "              loss_fn = loss_fn,\n",
        "              accuracy_fn = accuracy_fn,\n",
        "              device = device)\n",
        "\n",
        "train_time_end_on_cpu = timer()\n",
        "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
        "                                            end=train_time_end_on_cpu,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "id": "i1GJ7R2l2Y-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results # Trained on CPU"
      ],
      "metadata": {
        "id": "pjKypZRh3a9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Sometimes, depending on your data/hardware, you model might train faster on CPU than GPU.\n",
        "\n",
        "Why?\n",
        "1. It could be that the overhead for copying data/model to and from the GPU overeweights the compute benefits offered by the GPU.\n",
        "2. The hardware you're using has a better CPU in terms of compute capability than the GPU."
      ],
      "metadata": {
        "id": "BJZ2ph9l4A1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_train_time_model_0"
      ],
      "metadata": {
        "id": "LI5duUrK3lqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "def eval_model(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn):\n",
        "  \"\"\"Return a dictionary containing results of model predictions on data_loader.\"\"\"\n",
        "  loss, acc = 0, 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(data_loader):\n",
        "      # Make our data device agnostic\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Make predictions\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # Accumulate the loss and acc values per batch\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    # Scale loss and acc to find the average loss/acc per batch\n",
        "    loss /= len(data_loader)\n",
        "    acc /= len(data_loader)\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
        "          \"model_loss\": loss.item(),\n",
        "          \"model_acc\": acc}"
      ],
      "metadata": {
        "id": "WOtkytp64-2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_1 results dictionary\n",
        "model_1_results = eval_model(model=model_1,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "znqPbZ615fHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model 2: Building a Convolutional Neural Network\n",
        "This is very useful for Computer Vision.\n",
        "\n",
        "Go to CNN EXPLAINER to visually interact with and understand CNN hyperparameters like kernel size, stride, and padding."
      ],
      "metadata": {
        "id": "HyLA-Bdk5f9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CNN\n",
        "\n",
        "# Images -> Tensors -> Convolutional Layer -> ReLU layer -> Pooling layer -> output\n",
        "\n",
        "class FashionMNISTModelV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_shape: int,\n",
        "                 hidden_units: int,\n",
        "                 output_shape: int):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN 1\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, # There is also Conv1d and Conv3d for respective images\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # CNN 2\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size = 3,\n",
        "                      stride = 1,\n",
        "                      padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # Output Layer\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*7*7, # 7 is the output of conv_block_2. You can find it further below\n",
        "                      out_features=output_shape),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        #print(x.shape)\n",
        "        x = self.conv_block_2(x)\n",
        "        #print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        #print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Kfhcmui9TE2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "model_2 = FashionMNISTModelV2(input_shape=1, # If we have color images, we input 3 (RGB)\n",
        "                              hidden_units=10,\n",
        "                              output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "YljVQoFhY8Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "BzjxqPk4h3Qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_image_tensor = torch.rand(size = (1, 28, 28))\n",
        "rand_image_tensor.shape"
      ],
      "metadata": {
        "id": "Q6D-Id7gkCrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2(rand_image_tensor.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "WUd5Ykjdh9iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stepping through nn.Conv2d()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a batch of images\n",
        "images = torch.randn(size=(32, 3, 64, 64))\n",
        "test_image = images[0]\n",
        "\n",
        "print(f\"Image batch shape: {image.shape}\")\n",
        "print(f\"Sample image shape: {test_image.shape}\")\n",
        "print(f\"Test image:\\n{ test_image}\")"
      ],
      "metadata": {
        "id": "T7-nFYGyZZ87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image.shape"
      ],
      "metadata": {
        "id": "o_IWPHt8bOyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a single conv2d layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, # color channels in image data\n",
        "                        out_channels=10,\n",
        "                        kernel_size=3, # can also be (3, 3)\n",
        "                        stride=1,\n",
        "                        padding=1)\n",
        "\n",
        "# Passing the data through conv_layer\n",
        "conv_output = conv_layer(test_image)\n",
        "conv_output"
      ],
      "metadata": {
        "id": "d2uF4GKoa3JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stepping through nn.MaxPool2d()\n",
        "\n",
        "# Create a sample max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass data through just the conv layer\n",
        "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
        "print(f\"Test image shape through conv layer:\\n{test_image_through_conv.shape}\")\n",
        "\n",
        "# Pass data through max_pool_layer\n",
        "text_image_through_conv_and_maxpool = max_pool_layer(test_image_through_conv)\n",
        "print(f\"Test image shape through maxpool layer:\\n{text_image_through_conv_and_maxpool.shape}\")"
      ],
      "metadata": {
        "id": "4Uifbs1DckXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Create a random tensor with a similar number of dimensions.\n",
        "random_tensor = torch.randn(size=(1, 1, 2, 2))\n",
        "print(f\"Random tensor:\\n{random_tensor}\")\n",
        "print(f\"Random tensor shape:\\n{random_tensor.shape}\")\n",
        "\n",
        "# Create a max pool layer\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "# Pass the random tensor through the max pool layer\n",
        "max_pool_tensor = max_pool_layer(random_tensor)\n",
        "\n",
        "# Print the results\n",
        "print(f\"\\nMax pool tensor:\\n{max_pool_tensor}\")\n",
        "print(f\"\\nMax pool tensor shape:\\n{max_pool_tensor.shape}\")\n",
        "random_tensor"
      ],
      "metadata": {
        "id": "0-xhQpKZcogA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import accuracy_fn\n",
        "\n",
        "# Loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(params=model_2.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "qsgK4uOufy79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and Testing model_2 using functions\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Measure Time\n",
        "from timeit import default_timer as timer\n",
        "train_time_start_model_2 = timer()\n",
        "\n",
        "# Set epochs\n",
        "epochs = 3\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    train_step(model = model_2,\n",
        "               data_loader = train_dataloader,\n",
        "               loss_fn = loss_fn,\n",
        "               optimizer = optimizer,\n",
        "               accuracy_fn = accuracy_fn,\n",
        "               device = device)\n",
        "\n",
        "    test_step(model = model_2,\n",
        "              data_loader = test_dataloader,\n",
        "              loss_fn = loss_fn,\n",
        "              accuracy_fn = accuracy_fn,\n",
        "              device = device)\n",
        "\n",
        "train_time_end_model_2 = timer()\n",
        "total_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n",
        "                                            end=train_time_end_model_2,\n",
        "                                            device=device)"
      ],
      "metadata": {
        "id": "vxpoVM6_k3SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model_2 results\n",
        "model_2_results = eval_model(model=model_2,\n",
        "                             data_loader=test_dataloader,\n",
        "                             loss_fn=loss_fn,\n",
        "                             accuracy_fn=accuracy_fn)\n",
        "model_2_results # This would be better with better hardware GPUs"
      ],
      "metadata": {
        "id": "Vq9DEEvMlgUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_results"
      ],
      "metadata": {
        "id": "KTC3iXpGmLok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Compare model results and training time"
      ],
      "metadata": {
        "id": "oG1Id-OmmO6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compare_results = pd.DataFrame(data=[model_0_results,\n",
        "                                     model_1_results,\n",
        "                                     model_2_results])\n",
        "compare_results"
      ],
      "metadata": {
        "id": "aUxj1PCVmgQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training time to results comparison\n",
        "compare_results[\"model_training_time\"] = [total_train_time_model_0,\n",
        "                                         total_train_time_model_1,\n",
        "                                         total_train_time_model_2]\n",
        "compare_results"
      ],
      "metadata": {
        "id": "1T2rj82nmnqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our results\n",
        "compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\n",
        "plt.xlabel(\"accuracy (%)\")\n",
        "plt.ylabel(\"Model\")"
      ],
      "metadata": {
        "id": "YbRer5dvnGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model: torch.nn.Module,\n",
        "                    data: list,\n",
        "                    device: torch.device = device):\n",
        "  pred_probs = []\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for sample in data:\n",
        "      # Send sample to target device\n",
        "      sample = torch.unsqueeze(sample, dim = 0).to(device)\n",
        "\n",
        "      # Forward Pass (outputs logits)\n",
        "      pred_logit = model(sample)\n",
        "\n",
        "      # Prediction probabilities\n",
        "      pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "\n",
        "      # Get pred_prob off GPU for further calculations\n",
        "      pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "  # Stack the pred_probs to turn list into tensor\n",
        "  return torch.stack(pred_probs)"
      ],
      "metadata": {
        "id": "hjGALe0NnmQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "  test_samples.append(sample)\n",
        "  test_labels.append(label)\n",
        "\n",
        "# View the first sample shape\n",
        "test_samples[0].shape"
      ],
      "metadata": {
        "id": "KWkveic-3SVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_samples[0].squeeze(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "Igeekz3j3r2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "pred_probs = make_predictions(model=model_2,\n",
        "                              data=test_samples)\n",
        "\n",
        "# View the first two prediction probabilities\n",
        "pred_probs[:2]"
      ],
      "metadata": {
        "id": "93Ug4WyQ3vQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels"
      ],
      "metadata": {
        "id": "oCoQ_NYk4BmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_classes = pred_probs.argmax(dim=1)\n",
        "pred_classes"
      ],
      "metadata": {
        "id": "_lMOfCon4L1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize = (9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  # Create subplot\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "\n",
        "  # Plot target image\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "\n",
        "  # Find the prediction label\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "\n",
        "  # Get the truth label (in text form)\n",
        "  truth_label = class_names[test_labels[i]]\n",
        "\n",
        "  # Create a title for the plot\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "\n",
        "  # Check for equality between pred and truth and change color of title text.\n",
        "  if pred_label == truth_label:\n",
        "    plt.title(title_text, fontsize=10, c=\"g\")\n",
        "  else:\n",
        "    plt.title(title_text, fontsize=10, c=\"r\")\n",
        "  plt.axis(False)"
      ],
      "metadata": {
        "id": "guDlVMB74Z6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Making a Confusion Matrix for further prediction evaluation\n",
        "\n",
        "1. Make predictions with our trained model on the test dataset.\n",
        "2. Make a confusion matrix `torchmetrics.ConfusionMatrix`\n",
        "3. Plot the confusion matrix using `mixtend.plotting.plot_confusion_matrix()`"
      ],
      "metadata": {
        "id": "54YftOdq9jQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Make predictions with trained model\n",
        "y_preds = []\n",
        "model_2.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc = \"Making predictions...\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    # Forward Pass\n",
        "    y_logit = model_2(X)\n",
        "    # Get prediction probabilities from lgits\n",
        "    y_pred = torch.softmax(y_logit.squeeze(), dim=0).argmax(dim=1)\n",
        "    # Append to list and put predictions on cpu\n",
        "    y_preds.append(y_pred.cpu())\n",
        "\n",
        "# print(y_preds)\n",
        "\n",
        "# Concatenate list of predictions into a tensor\n",
        "y_pred_tensor = torch.cat(y_preds)\n",
        "y_pred_tensor[:10]"
      ],
      "metadata": {
        "id": "OP7kMMqr_rk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if required packages are installed and if not, install them\n",
        "try:\n",
        "  import torchmetrics, mlxtend\n",
        "  print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "  assert int(mlxtend.__version__.split(\".\")[1] >= 19, \"Version should be 0.19.0 or higher\")\n",
        "except:\n",
        "  !pip install -q torchmetrics -U mlxtend\n",
        "  import torchmetrics, mlxtend\n",
        "  print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "9l_LyKcdBn60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlxtend\n",
        "mlxtend.__version__"
      ],
      "metadata": {
        "id": "R5zErWDg-xVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_tensor[:10]"
      ],
      "metadata": {
        "id": "NWOdLiqgDvjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.targets[:10]"
      ],
      "metadata": {
        "id": "XgzdPVOdDymB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# Setup Confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names),\n",
        "                          task=\"multiclass\")\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=test_data.targets)\n",
        "\n",
        "# Plot Confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    figsize=(10, 7),\n",
        "    class_names=class_names,\n",
        "    show_normed=True\n",
        ") # Ideally, our confusion matrix should be a darkened diagonal with no other values."
      ],
      "metadata": {
        "id": "nCD_r8uE_p8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Save and load best performing model"
      ],
      "metadata": {
        "id": "30qAJHKRD0Zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create model directory path\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create model save path\n",
        "MODEL_NAME = \"fashion_mnist_model_2.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# Save the model state dict (the model's learned parameters)\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model_2.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "metadata": {
        "id": "MXb7h1BqFEKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new instance\n",
        "torch.manual_seed(42)\n",
        "\n",
        "loaded_model_2 = FashionMNISTModelV2(\n",
        "    input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)\n",
        ")\n",
        "\n",
        "# Load in the save state_dict()\n",
        "loaded_model_2.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
        "\n",
        "# Send the model to the target device\n",
        "loaded_model_2.to(device)"
      ],
      "metadata": {
        "id": "MAjpvFOAFdij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_results"
      ],
      "metadata": {
        "id": "2YFnYzdrGi4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate loaded model\n",
        "torch.manual_seed(42)\n",
        "loaded_model_2_results = eval_model(model=loaded_model_2,\n",
        "                                    data_loader=test_dataloader,\n",
        "                                    loss_fn=loss_fn,\n",
        "                                    accuracy_fn=accuracy_fn)\n",
        "loaded_model_2_results # similar results as moddel_2"
      ],
      "metadata": {
        "id": "RhDHUFxsGfAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsQtBFMDHA29"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}